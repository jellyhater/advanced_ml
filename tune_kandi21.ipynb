{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "befc02ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db9082ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/jovyan/.imgenv-kand21-training-0/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/jovyan/.imgenv-kand21-training-0/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: clip in ./.imgenv-kand21-training-0/lib/python3.7/site-packages (1.0)\n",
      "Requirement already satisfied: tqdm in /home/user/conda/lib/python3.7/site-packages (from clip) (4.62.3)\n",
      "Collecting torch\n",
      "  Using cached torch-1.13.1-cp37-cp37m-manylinux1_x86_64.whl (887.5 MB)\n",
      "Requirement already satisfied: torchvision in ./.imgenv-kand21-training-0/lib/python3.7/site-packages (from clip) (0.14.1)\n",
      "Requirement already satisfied: regex in /home/user/conda/lib/python3.7/site-packages (from clip) (2022.1.18)\n",
      "Requirement already satisfied: ftfy in ./.imgenv-kand21-training-0/lib/python3.7/site-packages (from clip) (6.1.1)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /home/user/conda/lib/python3.7/site-packages (from ftfy->clip) (0.2.5)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in ./.imgenv-kand21-training-0/lib/python3.7/site-packages (from torch->clip) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in ./.imgenv-kand21-training-0/lib/python3.7/site-packages (from torch->clip) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in ./.imgenv-kand21-training-0/lib/python3.7/site-packages (from torch->clip) (8.5.0.96)\n",
      "Requirement already satisfied: typing-extensions in /home/user/conda/lib/python3.7/site-packages (from torch->clip) (4.0.1)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in ./.imgenv-kand21-training-0/lib/python3.7/site-packages (from torch->clip) (11.10.3.66)\n",
      "Requirement already satisfied: setuptools in ./.imgenv-kand21-training-0/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch->clip) (67.6.1)\n",
      "Requirement already satisfied: wheel in ./.imgenv-kand21-training-0/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch->clip) (0.40.0)\n",
      "Requirement already satisfied: numpy in /home/user/conda/lib/python3.7/site-packages (from torchvision->clip) (1.21.5)\n",
      "Requirement already satisfied: requests in /home/user/conda/lib/python3.7/site-packages (from torchvision->clip) (2.27.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/user/conda/lib/python3.7/site-packages (from torchvision->clip) (9.0.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/user/conda/lib/python3.7/site-packages (from requests->torchvision->clip) (2.0.11)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/user/conda/lib/python3.7/site-packages (from requests->torchvision->clip) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/user/conda/lib/python3.7/site-packages (from requests->torchvision->clip) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/user/conda/lib/python3.7/site-packages (from requests->torchvision->clip) (3.3)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/jovyan/.imgenv-kand21-training-0/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: torch\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/jovyan/.imgenv-kand21-training-0/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed torch-1.13.1\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/jovyan/.imgenv-kand21-training-0/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/jovyan/.imgenv-kand21-training-0/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/jovyan/.imgenv-kand21-training-0/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "25f414ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/nvidia/lib:/usr/local/nvidia/lib64\r\n"
     ]
    }
   ],
   "source": [
    "!export LD_LIBRARY_PATH=/usr/local/nvidia/lib64\n",
    "!echo $LD_LIBRARY_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9de670bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/nvidia/lib:/usr/local/nvidia/lib64\r\n"
     ]
    }
   ],
   "source": [
    "!echo $LD_LIBRARY_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "475723b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "/home/user/conda/envs/test_env/lib/python3.8/site-packages/torch/lib/libtorch_cuda.so: symbol cudaGraphDebugDotPrint version libcudart.so.11.0 not defined in file libcudart.so.11.0 with link time reference",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n",
      "File \u001b[0;32m/home/user/conda/envs/test_env/lib/python3.8/site-packages/torch/__init__.py:229\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m USE_GLOBAL_DEPS:\n\u001b[1;32m    228\u001b[0m         _load_global_deps()\n\u001b[0;32m--> 229\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;66;03m# Appease the type checker; ordinarily this binding is inserted by the\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;66;03m# torch._C module initialization code in C\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
      "\u001b[0;31mImportError\u001b[0m: /home/user/conda/envs/test_env/lib/python3.8/site-packages/torch/lib/libtorch_cuda.so: symbol cudaGraphDebugDotPrint version libcudart.so.11.0 not defined in file libcudart.so.11.0 with link time reference"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "os.environ['LD_LIBRARY_PATH'] = '/usr/local/nvidia/lib64'\n",
    "import random\n",
    "from copy import deepcopy\n",
    "\n",
    "import PIL\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "from matplotlib import pyplot as plt\n",
    "from omegaconf.dictconfig import DictConfig\n",
    "from huggingface_hub import hf_hub_url, cached_download\n",
    "\n",
    "\n",
    "import sys\n",
    "from kandinsky2.model.model_creation import create_model, create_gaussian_diffusion\n",
    "from kandinsky2.train_utils.train_module_pl2_1 import Decoder\n",
    "\n",
    "from kandinsky2.train_utils.data.dataset_prior import create_loader\n",
    "\n",
    "from kandinsky2.model.utils import get_obj_from_str\n",
    "from kandinsky2.train_utils.trainer_prior import train_prior\n",
    "from kandinsky2.model.resample import UniformSampler\n",
    "from kandinsky2.model.prior import PriorDiffusionModel, CustomizedTokenizer\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from kandinsky2.train_utils.utils import freeze_decoder\n",
    "\n",
    "from kandinsky2.model.text_encoders import TextEncoder\n",
    "from kandinsky2.vqgan.autoencoder import VQModelInterface, AutoencoderKL, MOVQ\n",
    "from kandinsky2.train_utils.trainer_2_1_uclip import train_unclip\n",
    "from kandinsky2.model.resample import UniformSampler\n",
    "from omegaconf import OmegaConf\n",
    "import clip\n",
    "\n",
    "from kandinsky2 import CONFIG_2_1, Kandinsky2_1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da278aba",
   "metadata": {},
   "source": [
    "### Training parameters \n",
    "#### Necessarily fill initializer_token, placeholder_token, data_root and out_folder. You can change other parameters if you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "77610d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "task_type = 'text2img'\n",
    "cache_root = '/tmp/kandinsky2'\n",
    "\n",
    "# Fill here -------------------------------------------------------\n",
    "#initializer_token = 'food'\n",
    "#placeholder_token = 'khinkali'\n",
    "#data_root = 'datasets/khinkali'\n",
    "#out_folder = 'tmp/outputs'\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "os.makedirs(out_folder, exist_ok=True)\n",
    "\n",
    "img_size = 512\n",
    "epochs = 3000\n",
    "log_image_frequency = 500 # -1 disable image logging\n",
    "log_embed_frequency = 500\n",
    "\n",
    "lr = 1e-6\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "weight_decay = 1e-2\n",
    "epsilon = 1e-08\n",
    "\n",
    "num_workers = 0\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1041a3f5",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "baac4616",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_models_if_not_exist(\n",
    "    task_type=\"text2img\",\n",
    "    cache_dir=\"/tmp/kandinsky2\",\n",
    "    use_auth_token=None,\n",
    "):\n",
    "    cache_dir = os.path.join(cache_dir, \"2_1\")\n",
    "    if task_type == \"text2img\":\n",
    "        model_name = \"decoder_fp16.ckpt\"\n",
    "        config_file_url = hf_hub_url(repo_id=\"sberbank-ai/Kandinsky_2.1\", filename=model_name)\n",
    "    elif task_type == \"inpainting\":\n",
    "        model_name = \"inpainting_fp16.ckpt\"\n",
    "        config_file_url = hf_hub_url(repo_id=\"sberbank-ai/Kandinsky_2.1\", filename=model_name)\n",
    "    cached_download(\n",
    "        config_file_url,\n",
    "        cache_dir=cache_dir,\n",
    "        force_filename=model_name,\n",
    "        use_auth_token=use_auth_token,\n",
    "    )\n",
    "    prior_name = \"prior_fp16.ckpt\"\n",
    "    config_file_url = hf_hub_url(repo_id=\"sberbank-ai/Kandinsky_2.1\", filename=prior_name)\n",
    "    cached_download(\n",
    "        config_file_url,\n",
    "        cache_dir=cache_dir,\n",
    "        force_filename=prior_name,\n",
    "        use_auth_token=use_auth_token,\n",
    "    )\n",
    "    cache_dir_text_en = os.path.join(cache_dir, \"text_encoder\")\n",
    "    for name in [\n",
    "        \"config.json\",\n",
    "        \"pytorch_model.bin\",\n",
    "        \"sentencepiece.bpe.model\",\n",
    "        \"special_tokens_map.json\",\n",
    "        \"tokenizer.json\",\n",
    "        \"tokenizer_config.json\",\n",
    "    ]:\n",
    "        config_file_url = hf_hub_url(repo_id=\"sberbank-ai/Kandinsky_2.1\", filename=f\"text_encoder/{name}\")\n",
    "        cached_download(\n",
    "            config_file_url,\n",
    "            cache_dir=cache_dir_text_en,\n",
    "            force_filename=name,\n",
    "            use_auth_token=use_auth_token,\n",
    "        )\n",
    "    config_file_url = hf_hub_url(repo_id=\"sberbank-ai/Kandinsky_2.1\", filename=\"movq_final.ckpt\")\n",
    "    cached_download(\n",
    "        config_file_url,\n",
    "        cache_dir=cache_dir,\n",
    "        force_filename=\"movq_final.ckpt\",\n",
    "        use_auth_token=use_auth_token,\n",
    "    )\n",
    "    config_file_url = hf_hub_url(repo_id=\"sberbank-ai/Kandinsky_2.1\", filename=\"ViT-L-14_stats.th\")\n",
    "    cached_download(\n",
    "        config_file_url,\n",
    "        cache_dir=cache_dir,\n",
    "        force_filename=\"ViT-L-14_stats.th\",\n",
    "        use_auth_token=use_auth_token,\n",
    "    )\n",
    "    \n",
    "def add_noise(original_samples, noise, timesteps):\n",
    "    num_diffusion_timesteps = 1000\n",
    "    scale = 1000 / num_diffusion_timesteps\n",
    "    beta_start = scale * 0.00085\n",
    "    beta_end = scale * 0.012\n",
    "        \n",
    "    betas = torch.linspace(beta_start, beta_end, num_diffusion_timesteps, dtype=original_samples.dtype)\n",
    "    alphas = 1.0 - betas\n",
    "    alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "        \n",
    "    alphas_cumprod = alphas_cumprod.to(device=original_samples.device, dtype=original_samples.dtype)\n",
    "    timesteps = timesteps.to(original_samples.device)\n",
    "\n",
    "    sqrt_alpha_prod = alphas_cumprod[timesteps] ** 0.5\n",
    "    sqrt_alpha_prod = sqrt_alpha_prod.flatten()\n",
    "    while len(sqrt_alpha_prod.shape) < len(original_samples.shape):\n",
    "        sqrt_alpha_prod = sqrt_alpha_prod.unsqueeze(-1)\n",
    "\n",
    "    sqrt_one_minus_alpha_prod = (1 - alphas_cumprod[timesteps]) ** 0.5\n",
    "    sqrt_one_minus_alpha_prod = sqrt_one_minus_alpha_prod.flatten()\n",
    "    while len(sqrt_one_minus_alpha_prod.shape) < len(original_samples.shape):\n",
    "        sqrt_one_minus_alpha_prod = sqrt_one_minus_alpha_prod.unsqueeze(-1)\n",
    "\n",
    "    noisy_samples = sqrt_alpha_prod * original_samples + sqrt_one_minus_alpha_prod * noise\n",
    "    return noisy_samples\n",
    "\n",
    "def generate_clip_emb(model,\n",
    "        prompt,\n",
    "        batch_size=1,\n",
    "        prior_cf_scale=1,\n",
    "        prior_steps=\"5\",\n",
    "        negative_prior_prompt=\"\",\n",
    "    ):\n",
    "    prompts_batch = [prompt for _ in range(batch_size)]\n",
    "    prior_cf_scales_batch = [prior_cf_scale] * len(prompts_batch)\n",
    "    prior_cf_scales_batch = torch.tensor(prior_cf_scales_batch, device=model.device)\n",
    "    max_txt_length = model.prior.model.text_ctx\n",
    "    tok, mask = model.tokenizer2.padded_tokens_and_mask(\n",
    "        prompts_batch, max_txt_length\n",
    "    )\n",
    "    cf_token, cf_mask = model.tokenizer2.padded_tokens_and_mask(\n",
    "        [negative_prior_prompt], max_txt_length\n",
    "    )\n",
    "    if not (cf_token.shape == tok.shape):\n",
    "        cf_token = cf_token.expand(tok.shape[0], -1)\n",
    "        cf_mask = cf_mask.expand(tok.shape[0], -1)\n",
    "    tok = torch.cat([tok, cf_token], dim=0)\n",
    "    mask = torch.cat([mask, cf_mask], dim=0)\n",
    "    tok, mask = tok.to(device=model.device), mask.to(device=model.device)\n",
    "    x = model.clip_model.token_embedding(tok).type(model.clip_model.dtype)\n",
    "    x = x + model.clip_model.positional_embedding.type(model.clip_model.dtype)\n",
    "    x = x.permute(1, 0, 2)  # NLD -> LND|\n",
    "    x = model.clip_model.transformer(x)\n",
    "    x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "    x = model.clip_model.ln_final(x).type(model.clip_model.dtype)\n",
    "    txt_feat_seq = x\n",
    "    txt_feat = (x[torch.arange(x.shape[0]), tok.argmax(dim=-1)] @ model.clip_model.text_projection)\n",
    "    txt_feat, txt_feat_seq = txt_feat.float().to(model.device), txt_feat_seq.float().to(model.device)\n",
    "    \n",
    "    img_feat = model.prior(\n",
    "        txt_feat,\n",
    "        txt_feat_seq,\n",
    "        mask,\n",
    "        prior_cf_scales_batch,\n",
    "        timestep_respacing=prior_steps,\n",
    "    )\n",
    "    return img_feat.to(model.model_dtype)\n",
    "\n",
    "\n",
    "def save_embeds(model, save_path, placeholder_token, t1_place_token_id, t2_place_token_id):\n",
    "    t1_embeds = model.text_encoder.model.transformer.get_input_embeddings().weight[t1_place_token_id]\n",
    "    t2_embeds = model.clip_model.token_embedding.weight[t2_place_token_id]\n",
    "    learned_embeds_dict = {\n",
    "        't1': {\n",
    "            placeholder_token: t1_embeds.cpu().detach(), \n",
    "        },\n",
    "        't2':{\n",
    "            placeholder_token: t2_embeds.cpu().detach(),\n",
    "        },\n",
    "    }\n",
    "    torch.save(learned_embeds_dict, save_path)\n",
    "    \n",
    "    \n",
    "def save_images(model, save_path, placeholder_token, img_size=512):\n",
    "    gen_images = model.generate_text2img(\n",
    "            f\"a photo of a {placeholder_token}\",\n",
    "            num_steps=50, \n",
    "            batch_size=4, \n",
    "            guidance_scale=7.5,\n",
    "            h=img_size, \n",
    "            w=img_size,\n",
    "            sampler=\"p_sampler\", \n",
    "            prior_cf_scale=4,\n",
    "            prior_steps=\"5\",\n",
    "        )\n",
    "\n",
    "    gen_images = np.hstack([np.array(img) for img in gen_images])\n",
    "    Image.fromarray(gen_images).save(save_path)\n",
    "\n",
    "def check_tokens_is_valid(model, placeholder_token, initializer_token):\n",
    "    print(\"Check tokens...\")\n",
    "    if placeholder_token in model.tokenizer2.encoder: \n",
    "        raise ValueError(f\"Word {placeholder_token} exists in tokenizer2. Please select another word.\")\n",
    "\n",
    "    if initializer_token not in model.tokenizer2.encoder:  \n",
    "        raise ValueError(f\"Word {initializer_token} doesn't exist in tokenizer2. Please select another word.\")\n",
    "\n",
    "    if len(model.tokenizer1.encode(placeholder_token)) == 3: \n",
    "        raise ValueError(f\"Word {placeholder_token} exists in tokenizer1. Please select another word.\")\n",
    "\n",
    "    if len(model.tokenizer1.encode(initializer_token)) != 3: \n",
    "        raise ValueError(f\"Word {initializer_token} doesn't exists in tokenizer1. Please select another word.\")\n",
    "    print(\"Selected tokens are correct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16fff74",
   "metadata": {},
   "source": [
    "Trainer prior <a href=\"https://github.com/ai-forever/Kandinsky-2/tree/main/kandinsky2/train_utils\">copied here</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5fee7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(tok, clip_model):\n",
    "    with torch.no_grad():\n",
    "        x = clip_model.token_embedding(tok).type(clip_model.dtype)\n",
    "        x = x + clip_model.positional_embedding.type(clip_model.dtype)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x = clip_model.transformer(x)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        txt_feat_seq = x\n",
    "        txt_feat = x[torch.arange(x.shape[0]), tok.argmax(dim=-1)] @ clip_model.text_projection\n",
    "        txt_feat, txt_feat_seq = txt_feat.float(), txt_feat_seq.float()\n",
    "        return txt_feat, txt_feat_seq\n",
    "\n",
    "def encode_image(image, clip_model, clip_mean, clip_std):\n",
    "    with torch.no_grad():\n",
    "        return (clip_model.encode_image(image).float() - clip_mean) / clip_std\n",
    "\n",
    "def train_prior(model, diffusion,\n",
    "                  clip_model, optimizer,\n",
    "                  lr_scheduler=None, schedule_sampler=None, \n",
    "                  train_loader=None, val_loader=None,\n",
    "                  num_epochs=2, save_every=1000, save_name='model',\n",
    "                  save_path='', device='cuda:0'):\n",
    "    train_step = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        progress = tqdm(total=len(train_loader), desc='finetuning goes brrr')\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            image, cond = batch\n",
    "            image = image.to(device)\n",
    "            for key in cond.keys():\n",
    "                cond[key] = cond[key].to(device)\n",
    "            image = encode_image(image, clip_model, model.clip_mean.to(device), model.clip_std.to(device))\n",
    "            txt_feat, txt_feat_seq = encode_text(cond['tokens'], clip_model)\n",
    "            cond = {\n",
    "            \"text_emb\": txt_feat,\n",
    "            \"text_enc\": txt_feat_seq,\n",
    "            \"mask\": cond['mask'],\n",
    "            \"causal_mask\": model.causal_mask,\n",
    "            }\n",
    "            t, weights = schedule_sampler.sample(image.shape[0], image.device)\n",
    "            compute_losses = functools.partial(\n",
    "                    diffusion.training_losses,\n",
    "                    model.model,\n",
    "                    image,\n",
    "                    t,\n",
    "                    model_kwargs=cond,\n",
    "                )\n",
    "            losses = compute_losses()\n",
    "            loss = losses[\"loss\"].mean()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if lr_scheduler is not None:\n",
    "                lr_scheduler.step()\n",
    "            train_step += 1\n",
    "            if train_step % save_every == 0:\n",
    "                torch.save(model.state_dict(), os.path.join(save_path, save_name + str(train_step) + '.ckpt'))\n",
    "            progress.update()\n",
    "            progress.set_postfix({\"loss\": loss.item()})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b05a5ea",
   "metadata": {},
   "source": [
    "Trainer unclip <a href=\"https://github.com/ai-forever/Kandinsky-2/blob/main/kandinsky2/train_utils/trainer_2_1_uclip.py\">copied here</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756c95ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_image(batch, image_encoder, scale=1):\n",
    "    with torch.no_grad():\n",
    "        batch = batch.half()\n",
    "        batch = image_encoder.encode(batch)\n",
    "        batch = batch * scale\n",
    "    return batch.float()\n",
    "\n",
    "def prepare_cond(cond, text_encoder, clip_model):\n",
    "    mask = None\n",
    "    new_cond = {}\n",
    "    for key in cond.keys():\n",
    "        if key not in ['tokens', 'mask', 'clip_image']:\n",
    "            new_cond[key] = cond[key]\n",
    "    if 'mask' in cond:\n",
    "        mask = cond['mask']\n",
    "    with torch.no_grad():\n",
    "        new_cond['image_emb'] = clip_model.encode_image(cond['clip_image']).float()\n",
    "    with torch.no_grad():\n",
    "        new_cond['full_emb'], new_cond['pooled_emb'] = text_encoder(\n",
    "                    cond['tokens'].long(), mask)\n",
    "        new_cond['full_emb'] = new_cond['full_emb'].float()\n",
    "        new_cond['pooled_emb'] = new_cond['pooled_emb'].float()\n",
    "    del cond\n",
    "    return new_cond\n",
    "\n",
    "def train_unclip(unet, diffusion, image_encoder,\n",
    "                  clip_model, text_encoder, optimizer,\n",
    "                  lr_scheduler=None, schedule_sampler=None, \n",
    "                  train_loader=None, val_loader=None, scale=1,\n",
    "                  num_epochs=2, save_every=1000, save_name='model',\n",
    "                  save_path='',  inpainting=False, device='cuda:0'):\n",
    "    train_step = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        progress = tqdm(total=len(train_loader), desc='finetuning goes brrr')\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            image, cond = batch\n",
    "            image = image.to(device)\n",
    "            for key in cond.keys():\n",
    "                cond[key] = cond[key].to(device)\n",
    "            image = prepare_image(image, image_encoder, scale=scale)\n",
    "            if inpainting:\n",
    "                image_mask = get_image_mask(image.shape[0], image.shape[-2:])\n",
    "                image_mask = image_mask.to(image.device).unsqueeze(1).to(image.dtype)\n",
    "                image_mask = 1. - image_mask\n",
    "                cond['inpaint_image'] = image * image_mask\n",
    "                cond['inpaint_mask'] = image_mask\n",
    "            cond = prepare_cond(cond, text_encoder, clip_model)\n",
    "            t, weights = schedule_sampler.sample(image.shape[0], image.device)\n",
    "            compute_losses = functools.partial(\n",
    "                    diffusion.training_losses,\n",
    "                    unet,\n",
    "                    image,\n",
    "                    t,\n",
    "                    model_kwargs=cond,\n",
    "                )\n",
    "            losses = compute_losses()\n",
    "            loss = losses[\"loss\"].mean()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if lr_scheduler is not None:\n",
    "                lr_scheduler.step()\n",
    "            train_step += 1\n",
    "            if train_step % save_every == 0:\n",
    "                torch.save(unet.state_dict(), os.path.join(save_path, save_name + str(train_step) + '.ckpt'))\n",
    "            progress.update()\n",
    "            progress.set_postfix({\"loss\": loss.item()})\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e21911",
   "metadata": {},
   "source": [
    "### Dataset class functions\n",
    "Text templates and dataset class with small changes from <a href=\"https://github.com/huggingface/diffusers/tree/main/examples/textual_inversion\">diffusers</a> repo:  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecabd03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = \"datasets/tags_princess_mononoke.xlsx\"\n",
    "DEVICE = 'cuda'\n",
    "IMAGES_FOLDER = 'datasets/princess_mononoke'\n",
    "CHECKPOINT_PATH = \"checkpoints2/laputa_castle_2.pt\"\n",
    "SAVE_PATH = 'checkpoints2/princess_mononoke_2.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12fb787",
   "metadata": {},
   "outputs": [],
   "source": [
    "excel = pd.read_excel(CSV_PATH)\n",
    "excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ee39ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_crop(image):\n",
    "    width, height = image.size\n",
    "    new_size = min(width, height)\n",
    "    left = (width - new_size) / 2\n",
    "    top = (height - new_size) / 2\n",
    "    right = (width + new_size) / 2\n",
    "    bottom = (height + new_size) / 2\n",
    "    return image.crop((left, top, right, bottom))\n",
    "class kandinsky2Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, df, folder):\n",
    "        self.tokenizer1 = AutoTokenizer.from_pretrained('M-CLIP/XLM-Roberta-Large-Vit-B-16Plus')\n",
    "        self.tokenizer2 = AutoTokenizer.from_pretrained('google/mt5-small')\n",
    "        self.df = df\n",
    "        self.folder = folder\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        img_name = self.df['path'].iloc[item]\n",
    "        image = center_crop(Image.open(f'{self.folder}/{img_name}'))\n",
    "        image = image.resize((512, 512), resample=Image.BICUBIC, reducing_gap=1) # was 640\n",
    "        arr = np.array(image.convert(\"RGB\"))\n",
    "        arr = arr.astype(np.float32) / 127.5 - 1\n",
    "        text = self.df['text'].iloc[item]\n",
    "        if np.random.binomial(1, 0.1):\n",
    "            text = ''\n",
    "        text_encoding1 = self.tokenizer1(\n",
    "                text,\n",
    "                max_length=77,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                return_attention_mask=True,\n",
    "                add_special_tokens=True,\n",
    "                return_tensors=\"pt\")\n",
    "\n",
    "        tokens1 = text_encoding1['input_ids'][0]\n",
    "        mask1 = text_encoding1['attention_mask'][0]\n",
    "        text_encoding2 = self.tokenizer2(\n",
    "                text,\n",
    "                max_length=77,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                return_attention_mask=True,\n",
    "                add_special_tokens=True,\n",
    "                return_tensors=\"pt\")\n",
    "\n",
    "        tokens2 = text_encoding2['input_ids'][0]\n",
    "        mask2 = text_encoding2['attention_mask'][0]\n",
    "        \n",
    "        return np.transpose(arr, [2, 0, 1]), tokens1, mask1, tokens2, mask2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532186f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = kandinsky2Dataset(df, IMAGES_FOLDER)\n",
    "train_data = DataLoader(dataset, batch_size=2, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1565bd45",
   "metadata": {},
   "source": [
    "### Define config and create Kandinsky model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ff1d6ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making attention of type 'vanilla' with 512 in_channels\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n"
     ]
    }
   ],
   "source": [
    "config = DictConfig(deepcopy(CONFIG_2_1))\n",
    "\n",
    "cache_dir = os.path.join(cache_root, \"2_1\")\n",
    "\n",
    "config[\"model_config\"][\"up\"] = False\n",
    "config[\"model_config\"][\"use_fp16\"] = False\n",
    "config[\"model_config\"][\"inpainting\"] = False\n",
    "config[\"model_config\"][\"cache_text_emb\"] = False\n",
    "config[\"model_config\"][\"use_flash_attention\"] = False\n",
    "\n",
    "config[\"tokenizer_name\"] = os.path.join(cache_dir, \"text_encoder\")\n",
    "config[\"text_enc_params\"][\"model_path\"] = os.path.join(cache_dir, \"text_encoder\")\n",
    "config[\"prior\"][\"clip_mean_std_path\"] = os.path.join(cache_dir, \"ViT-L-14_stats.th\")\n",
    "config[\"image_enc_params\"][\"ckpt_path\"] = os.path.join(cache_dir, \"movq_final.ckpt\")\n",
    "\n",
    "model_path = os.path.join(cache_dir, \"decoder_fp16.ckpt\")\n",
    "prior_path = os.path.join(cache_dir, \"prior_fp16.ckpt\")\n",
    "\n",
    "download_models_if_not_exist(task_type=task_type, cache_dir=cache_root)\n",
    "\n",
    "model = Kandinsky2_1(config, model_path, prior_path, device, task_type=task_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "94d02689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check tokens...\n",
      "Selected tokens are correct\n"
     ]
    }
   ],
   "source": [
    "check_tokens_is_valid(model, placeholder_token, initializer_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "59a4eb52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num added tokens: 1\n",
      "Initializer ID: 15381 | Placeholder token ID: 250002\n"
     ]
    }
   ],
   "source": [
    "# Convert the initializer_token and placeholder_token to tokenizer1 ids\n",
    "num_added_tokens = model.tokenizer1.add_tokens(placeholder_token)\n",
    "print(f'Num added tokens: {num_added_tokens}')\n",
    "\n",
    "t1_init_token_id = model.tokenizer1.encode(initializer_token, add_special_tokens=False)[0]\n",
    "t1_place_token_id = model.tokenizer1.convert_tokens_to_ids(placeholder_token)\n",
    "\n",
    "model.text_encoder.model.transformer.resize_token_embeddings(len(model.tokenizer1))\n",
    "\n",
    "print(f'Initializer ID: {t1_init_token_id} | Placeholder token ID: {t1_place_token_id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "53ca859a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise new placeholder weights with the embeddings of the initializer token\n",
    "token_embeds = model.text_encoder.model.transformer.get_input_embeddings().weight.data\n",
    "token_embeds[t1_place_token_id] = token_embeds[t1_init_token_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fc7441ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encode placeholder token: 49408 | Decode placeholder word: khinkali\n"
     ]
    }
   ],
   "source": [
    "# Convert the initializer_token, placeholder_token to ids for tokenizer2\n",
    "# and add placeholder_token to tokenizer2\n",
    "t2p_index_to_add = len(model.tokenizer2.encoder)\n",
    "model.tokenizer2.encoder[placeholder_token] = t2p_index_to_add\n",
    "model.tokenizer2.decoder[t2p_index_to_add] = placeholder_token\n",
    "model.tokenizer2.cache[placeholder_token] = placeholder_token\n",
    "\n",
    "t2_place_token_id = model.tokenizer2.encode(placeholder_token)[0]\n",
    "t2_place_token_str = model.tokenizer2.decode([t2_place_token_id])\n",
    "\n",
    "print(f'Encode placeholder token: {t2_place_token_id} | Decode placeholder word: {t2_place_token_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2cefdf46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encode initializer token: 1559 | Decode initializer word: food \n",
      "T2 old vocab size: 49408 | T2 Embed size: 768\n",
      "T2 new vocab size: 49409\n"
     ]
    }
   ],
   "source": [
    "# 1.Convert the initializer_token and placeholder_token to tokenizer2 ids\n",
    "# 2.Create new embeddings \n",
    "# 3.Copy old weights to the new embeddings and initialize new token \n",
    "    \n",
    "t2_init_token_id = model.tokenizer2.encode(initializer_token)[0]\n",
    "t2_init_token_str = model.tokenizer2.decode([t2_init_token_id])\n",
    "print(f'Encode initializer token: {t2_init_token_id} | Decode initializer word: {t2_init_token_str}')\n",
    "\n",
    "old_vocab_size, t2_embed_size = model.clip_model.token_embedding.weight.shape\n",
    "print(f'T2 old vocab size: {old_vocab_size} | T2 Embed size: {t2_embed_size}')\n",
    "\n",
    "new_embed = nn.Embedding(old_vocab_size + 1, t2_embed_size).to(device)\n",
    "new_embed.weight.data[:old_vocab_size, :] = model.clip_model.token_embedding.weight.data.clone()\n",
    "new_embed.weight.data[t2_place_token_id, :] = new_embed.weight.data[t2_init_token_id, :]\n",
    "\n",
    "model.clip_model.token_embedding = deepcopy(new_embed)\n",
    "\n",
    "print(f'T2 new vocab size: {model.clip_model.token_embedding.weight.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "070ea6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Freeze all except embeddings\n",
    "model.image_encoder.requires_grad_(False)\n",
    "model.model.requires_grad_(False)\n",
    "model.prior.requires_grad_(False)\n",
    "\n",
    "model.clip_model.token_embedding.requires_grad_(True)\n",
    "model.clip_model.transformer.requires_grad_(False);\n",
    "\n",
    "model.text_encoder.model.transformer.get_input_embeddings().requires_grad_(True)\n",
    "model.text_encoder.model.transformer.embeddings.position_embeddings.requires_grad_(False)\n",
    "model.text_encoder.model.transformer.embeddings.token_type_embeddings.requires_grad_(False)\n",
    "model.text_encoder.model.transformer.encoder.requires_grad_(False)\n",
    "model.text_encoder.model.transformer.pooler.requires_grad_(False);\n",
    "model.text_encoder.model.LinearTransformation.requires_grad_(False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "082576b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure optimizer, dataset and dataloader\n",
    "optimizer = torch.optim.AdamW(\n",
    "    list(model.text_encoder.model.transformer.get_input_embeddings().parameters()) +\n",
    "    list(model.clip_model.token_embedding.parameters()),\n",
    "    lr=lr,\n",
    "    betas=(beta1, beta2),\n",
    "    weight_decay=weight_decay,\n",
    "    eps=epsilon,\n",
    ")\n",
    "\n",
    "dataset = TextualInversionDataset(\n",
    "    data_root=data_root,\n",
    "    placeholder_token=placeholder_token,\n",
    "    img_size=img_size,\n",
    "    center_crop=False,\n",
    ")\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7d03272c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save original embeddings from both models\n",
    "orig_t1_params = model.text_encoder.model.transformer.get_input_embeddings().weight.data.clone()\n",
    "orig_t2_params = model.clip_model.token_embedding.weight.data.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ea598875",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 499/3000 [22:54<1:56:27,  2.79s/it, loss=0.0126]  \n",
      "  0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▏         | 1/50 [00:00<00:07,  6.51it/s]\u001b[A\n",
      "  4%|▍         | 2/50 [00:00<00:07,  6.78it/s]\u001b[A\n",
      "  6%|▌         | 3/50 [00:00<00:06,  6.85it/s]\u001b[A\n",
      "  8%|▊         | 4/50 [00:00<00:06,  6.89it/s]\u001b[A\n",
      " 10%|█         | 5/50 [00:00<00:06,  6.91it/s]\u001b[A\n",
      " 12%|█▏        | 6/50 [00:00<00:06,  6.92it/s]\u001b[A\n",
      " 14%|█▍        | 7/50 [00:01<00:06,  6.93it/s]\u001b[A\n",
      " 16%|█▌        | 8/50 [00:01<00:06,  6.95it/s]\u001b[A\n",
      " 18%|█▊        | 9/50 [00:01<00:05,  6.96it/s]\u001b[A\n",
      " 20%|██        | 10/50 [00:01<00:05,  6.97it/s]\u001b[A\n",
      " 22%|██▏       | 11/50 [00:01<00:05,  6.98it/s]\u001b[A\n",
      " 24%|██▍       | 12/50 [00:01<00:05,  6.98it/s]\u001b[A\n",
      " 26%|██▌       | 13/50 [00:01<00:05,  6.97it/s]\u001b[A\n",
      " 28%|██▊       | 14/50 [00:02<00:05,  6.95it/s]\u001b[A\n",
      " 30%|███       | 15/50 [00:02<00:05,  6.95it/s]\u001b[A\n",
      " 32%|███▏      | 16/50 [00:02<00:04,  6.95it/s]\u001b[A\n",
      " 34%|███▍      | 17/50 [00:02<00:04,  6.95it/s]\u001b[A\n",
      " 36%|███▌      | 18/50 [00:02<00:04,  6.94it/s]\u001b[A\n",
      " 38%|███▊      | 19/50 [00:02<00:04,  6.95it/s]\u001b[A\n",
      " 40%|████      | 20/50 [00:02<00:04,  6.94it/s]\u001b[A\n",
      " 42%|████▏     | 21/50 [00:03<00:04,  6.94it/s]\u001b[A\n",
      " 44%|████▍     | 22/50 [00:03<00:04,  6.94it/s]\u001b[A\n",
      " 46%|████▌     | 23/50 [00:03<00:03,  6.96it/s]\u001b[A\n",
      " 48%|████▊     | 24/50 [00:03<00:03,  6.97it/s]\u001b[A\n",
      " 50%|█████     | 25/50 [00:03<00:03,  6.98it/s]\u001b[A\n",
      " 52%|█████▏    | 26/50 [00:03<00:03,  6.98it/s]\u001b[A\n",
      " 54%|█████▍    | 27/50 [00:03<00:03,  6.96it/s]\u001b[A\n",
      " 56%|█████▌    | 28/50 [00:04<00:03,  6.95it/s]\u001b[A\n",
      " 58%|█████▊    | 29/50 [00:04<00:03,  6.94it/s]\u001b[A\n",
      " 60%|██████    | 30/50 [00:04<00:02,  6.94it/s]\u001b[A\n",
      " 62%|██████▏   | 31/50 [00:04<00:02,  6.94it/s]\u001b[A\n",
      " 64%|██████▍   | 32/50 [00:04<00:02,  6.94it/s]\u001b[A\n",
      " 66%|██████▌   | 33/50 [00:04<00:02,  6.93it/s]\u001b[A\n",
      " 68%|██████▊   | 34/50 [00:04<00:02,  6.93it/s]\u001b[A\n",
      " 70%|███████   | 35/50 [00:05<00:02,  6.95it/s]\u001b[A\n",
      " 72%|███████▏  | 36/50 [00:05<00:02,  6.96it/s]\u001b[A\n",
      " 74%|███████▍  | 37/50 [00:05<00:01,  6.98it/s]\u001b[A\n",
      " 76%|███████▌  | 38/50 [00:05<00:01,  6.98it/s]\u001b[A\n",
      " 78%|███████▊  | 39/50 [00:05<00:01,  6.95it/s]\u001b[A\n",
      " 80%|████████  | 40/50 [00:05<00:01,  6.95it/s]\u001b[A\n",
      " 82%|████████▏ | 41/50 [00:05<00:01,  6.95it/s]\u001b[A\n",
      " 84%|████████▍ | 42/50 [00:06<00:01,  6.94it/s]\u001b[A\n",
      " 86%|████████▌ | 43/50 [00:06<00:01,  6.94it/s]\u001b[A\n",
      " 88%|████████▊ | 44/50 [00:06<00:00,  6.93it/s]\u001b[A\n",
      " 90%|█████████ | 45/50 [00:06<00:00,  6.93it/s]\u001b[A\n",
      " 92%|█████████▏| 46/50 [00:06<00:00,  6.94it/s]\u001b[A\n",
      " 94%|█████████▍| 47/50 [00:06<00:00,  6.93it/s]\u001b[A\n",
      " 96%|█████████▌| 48/50 [00:06<00:00,  6.94it/s]\u001b[A\n",
      " 98%|█████████▊| 49/50 [00:07<00:00,  6.93it/s]\u001b[A\n",
      "100%|██████████| 50/50 [00:07<00:00,  6.94it/s]\u001b[A\n",
      " 33%|███▎      | 999/3000 [45:53<1:30:53,  2.73s/it, loss=0.00997] \n",
      "  0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▏         | 1/50 [00:00<00:07,  6.56it/s]\u001b[A\n",
      "  4%|▍         | 2/50 [00:00<00:07,  6.78it/s]\u001b[A\n",
      "  6%|▌         | 3/50 [00:00<00:06,  6.84it/s]\u001b[A\n",
      "  8%|▊         | 4/50 [00:00<00:06,  6.90it/s]\u001b[A\n",
      " 10%|█         | 5/50 [00:00<00:06,  6.93it/s]\u001b[A\n",
      " 12%|█▏        | 6/50 [00:00<00:06,  6.94it/s]\u001b[A\n",
      " 14%|█▍        | 7/50 [00:01<00:06,  6.95it/s]\u001b[A\n",
      " 16%|█▌        | 8/50 [00:01<00:06,  6.97it/s]\u001b[A\n",
      " 18%|█▊        | 9/50 [00:01<00:05,  6.98it/s]\u001b[A\n",
      " 20%|██        | 10/50 [00:01<00:05,  6.97it/s]\u001b[A\n",
      " 22%|██▏       | 11/50 [00:01<00:05,  6.97it/s]\u001b[A\n",
      " 24%|██▍       | 12/50 [00:01<00:05,  6.96it/s]\u001b[A\n",
      " 26%|██▌       | 13/50 [00:01<00:05,  6.96it/s]\u001b[A\n",
      " 28%|██▊       | 14/50 [00:02<00:05,  6.95it/s]\u001b[A\n",
      " 30%|███       | 15/50 [00:02<00:05,  6.95it/s]\u001b[A\n",
      " 32%|███▏      | 16/50 [00:02<00:04,  6.96it/s]\u001b[A\n",
      " 34%|███▍      | 17/50 [00:02<00:04,  6.97it/s]\u001b[A\n",
      " 36%|███▌      | 18/50 [00:02<00:04,  6.96it/s]\u001b[A\n",
      " 38%|███▊      | 19/50 [00:02<00:04,  6.97it/s]\u001b[A\n",
      " 40%|████      | 20/50 [00:02<00:04,  6.97it/s]\u001b[A\n",
      " 42%|████▏     | 21/50 [00:03<00:04,  6.98it/s]\u001b[A\n",
      " 44%|████▍     | 22/50 [00:03<00:04,  6.97it/s]\u001b[A\n",
      " 46%|████▌     | 23/50 [00:03<00:03,  6.96it/s]\u001b[A\n",
      " 48%|████▊     | 24/50 [00:03<00:03,  6.96it/s]\u001b[A\n",
      " 50%|█████     | 25/50 [00:03<00:03,  6.96it/s]\u001b[A\n",
      " 52%|█████▏    | 26/50 [00:03<00:03,  6.94it/s]\u001b[A\n",
      " 54%|█████▍    | 27/50 [00:03<00:03,  6.96it/s]\u001b[A\n",
      " 56%|█████▌    | 28/50 [00:04<00:03,  6.95it/s]\u001b[A\n",
      " 58%|█████▊    | 29/50 [00:04<00:03,  6.96it/s]\u001b[A\n",
      " 60%|██████    | 30/50 [00:04<00:02,  6.95it/s]\u001b[A\n",
      " 62%|██████▏   | 31/50 [00:04<00:02,  6.95it/s]\u001b[A\n",
      " 64%|██████▍   | 32/50 [00:04<00:02,  6.94it/s]\u001b[A\n",
      " 66%|██████▌   | 33/50 [00:04<00:02,  6.94it/s]\u001b[A\n",
      " 68%|██████▊   | 34/50 [00:04<00:02,  6.93it/s]\u001b[A\n",
      " 70%|███████   | 35/50 [00:05<00:02,  6.93it/s]\u001b[A\n",
      " 72%|███████▏  | 36/50 [00:05<00:02,  6.93it/s]\u001b[A\n",
      " 74%|███████▍  | 37/50 [00:05<00:01,  6.94it/s]\u001b[A\n",
      " 76%|███████▌  | 38/50 [00:05<00:01,  6.94it/s]\u001b[A\n",
      " 78%|███████▊  | 39/50 [00:05<00:01,  6.94it/s]\u001b[A\n",
      " 80%|████████  | 40/50 [00:05<00:01,  6.93it/s]\u001b[A\n",
      " 82%|████████▏ | 41/50 [00:05<00:01,  6.93it/s]\u001b[A\n",
      " 84%|████████▍ | 42/50 [00:06<00:01,  6.93it/s]\u001b[A\n",
      " 86%|████████▌ | 43/50 [00:06<00:01,  6.92it/s]\u001b[A\n",
      " 88%|████████▊ | 44/50 [00:06<00:00,  6.92it/s]\u001b[A\n",
      " 90%|█████████ | 45/50 [00:06<00:00,  6.93it/s]\u001b[A\n",
      " 92%|█████████▏| 46/50 [00:06<00:00,  6.94it/s]\u001b[A\n",
      " 94%|█████████▍| 47/50 [00:06<00:00,  6.94it/s]\u001b[A\n",
      " 96%|█████████▌| 48/50 [00:06<00:00,  6.93it/s]\u001b[A\n",
      " 98%|█████████▊| 49/50 [00:07<00:00,  6.93it/s]\u001b[A\n",
      "100%|██████████| 50/50 [00:07<00:00,  6.94it/s]\u001b[A\n",
      " 50%|████▉     | 1499/3000 [1:08:58<1:08:14,  2.73s/it, loss=0.0188]  \n",
      "  0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▏         | 1/50 [00:00<00:07,  6.61it/s]\u001b[A\n",
      "  4%|▍         | 2/50 [00:00<00:07,  6.84it/s]\u001b[A\n",
      "  6%|▌         | 3/50 [00:00<00:06,  6.91it/s]\u001b[A\n",
      "  8%|▊         | 4/50 [00:00<00:06,  6.95it/s]\u001b[A\n",
      " 10%|█         | 5/50 [00:00<00:06,  6.97it/s]\u001b[A\n",
      " 12%|█▏        | 6/50 [00:00<00:06,  6.99it/s]\u001b[A\n",
      " 14%|█▍        | 7/50 [00:01<00:06,  6.99it/s]\u001b[A\n",
      " 16%|█▌        | 8/50 [00:01<00:06,  6.98it/s]\u001b[A\n",
      " 18%|█▊        | 9/50 [00:01<00:05,  6.97it/s]\u001b[A\n",
      " 20%|██        | 10/50 [00:01<00:05,  6.97it/s]\u001b[A\n",
      " 22%|██▏       | 11/50 [00:01<00:05,  6.97it/s]\u001b[A\n",
      " 24%|██▍       | 12/50 [00:01<00:05,  6.98it/s]\u001b[A\n",
      " 26%|██▌       | 13/50 [00:01<00:05,  6.97it/s]\u001b[A\n",
      " 28%|██▊       | 14/50 [00:02<00:05,  6.97it/s]\u001b[A\n",
      " 30%|███       | 15/50 [00:02<00:05,  6.95it/s]\u001b[A\n",
      " 32%|███▏      | 16/50 [00:02<00:04,  6.96it/s]\u001b[A\n",
      " 34%|███▍      | 17/50 [00:02<00:04,  6.97it/s]\u001b[A\n",
      " 36%|███▌      | 18/50 [00:02<00:04,  6.98it/s]\u001b[A\n",
      " 38%|███▊      | 19/50 [00:02<00:04,  6.97it/s]\u001b[A\n",
      " 40%|████      | 20/50 [00:02<00:04,  6.97it/s]\u001b[A\n",
      " 42%|████▏     | 21/50 [00:03<00:04,  6.97it/s]\u001b[A\n",
      " 44%|████▍     | 22/50 [00:03<00:04,  6.96it/s]\u001b[A\n",
      " 46%|████▌     | 23/50 [00:03<00:03,  6.95it/s]\u001b[A\n",
      " 48%|████▊     | 24/50 [00:03<00:03,  6.96it/s]\u001b[A\n",
      " 50%|█████     | 25/50 [00:03<00:03,  6.95it/s]\u001b[A\n",
      " 52%|█████▏    | 26/50 [00:03<00:03,  6.95it/s]\u001b[A\n",
      " 54%|█████▍    | 27/50 [00:03<00:03,  6.95it/s]\u001b[A\n",
      " 56%|█████▌    | 28/50 [00:04<00:03,  6.96it/s]\u001b[A\n",
      " 58%|█████▊    | 29/50 [00:04<00:03,  6.98it/s]\u001b[A\n",
      " 60%|██████    | 30/50 [00:04<00:02,  6.98it/s]\u001b[A\n",
      " 62%|██████▏   | 31/50 [00:04<00:02,  6.99it/s]\u001b[A\n",
      " 64%|██████▍   | 32/50 [00:04<00:02,  6.98it/s]\u001b[A\n",
      " 66%|██████▌   | 33/50 [00:04<00:02,  6.96it/s]\u001b[A\n",
      " 68%|██████▊   | 34/50 [00:04<00:02,  6.97it/s]\u001b[A\n",
      " 70%|███████   | 35/50 [00:05<00:02,  6.92it/s]\u001b[A\n",
      " 72%|███████▏  | 36/50 [00:05<00:02,  6.92it/s]\u001b[A\n",
      " 74%|███████▍  | 37/50 [00:05<00:01,  6.93it/s]\u001b[A\n",
      " 76%|███████▌  | 38/50 [00:05<00:01,  6.95it/s]\u001b[A\n",
      " 78%|███████▊  | 39/50 [00:05<00:01,  6.96it/s]\u001b[A\n",
      " 80%|████████  | 40/50 [00:05<00:01,  6.97it/s]\u001b[A\n",
      " 82%|████████▏ | 41/50 [00:05<00:01,  6.98it/s]\u001b[A\n",
      " 84%|████████▍ | 42/50 [00:06<00:01,  6.97it/s]\u001b[A\n",
      " 86%|████████▌ | 43/50 [00:06<00:01,  6.97it/s]\u001b[A\n",
      " 88%|████████▊ | 44/50 [00:06<00:00,  6.96it/s]\u001b[A\n",
      " 90%|█████████ | 45/50 [00:06<00:00,  6.97it/s]\u001b[A\n",
      " 92%|█████████▏| 46/50 [00:06<00:00,  6.97it/s]\u001b[A\n",
      " 94%|█████████▍| 47/50 [00:06<00:00,  6.96it/s]\u001b[A\n",
      " 96%|█████████▌| 48/50 [00:06<00:00,  6.94it/s]\u001b[A\n",
      " 98%|█████████▊| 49/50 [00:07<00:00,  6.95it/s]\u001b[A\n",
      "100%|██████████| 50/50 [00:07<00:00,  6.96it/s]\u001b[A\n",
      " 67%|██████▋   | 1999/3000 [1:32:08<44:59,  2.70s/it, loss=0.0446]    \n",
      "  0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▏         | 1/50 [00:00<00:07,  6.49it/s]\u001b[A\n",
      "  4%|▍         | 2/50 [00:00<00:07,  6.74it/s]\u001b[A\n",
      "  6%|▌         | 3/50 [00:00<00:06,  6.79it/s]\u001b[A\n",
      "  8%|▊         | 4/50 [00:00<00:06,  6.85it/s]\u001b[A\n",
      " 10%|█         | 5/50 [00:00<00:06,  6.87it/s]\u001b[A\n",
      " 12%|█▏        | 6/50 [00:00<00:06,  6.89it/s]\u001b[A\n",
      " 14%|█▍        | 7/50 [00:01<00:06,  6.91it/s]\u001b[A\n",
      " 16%|█▌        | 8/50 [00:01<00:06,  6.91it/s]\u001b[A\n",
      " 18%|█▊        | 9/50 [00:01<00:05,  6.91it/s]\u001b[A\n",
      " 20%|██        | 10/50 [00:01<00:05,  6.92it/s]\u001b[A\n",
      " 22%|██▏       | 11/50 [00:01<00:05,  6.91it/s]\u001b[A\n",
      " 24%|██▍       | 12/50 [00:01<00:05,  6.91it/s]\u001b[A\n",
      " 26%|██▌       | 13/50 [00:01<00:05,  6.91it/s]\u001b[A\n",
      " 28%|██▊       | 14/50 [00:02<00:05,  6.91it/s]\u001b[A\n",
      " 30%|███       | 15/50 [00:02<00:05,  6.92it/s]\u001b[A\n",
      " 32%|███▏      | 16/50 [00:02<00:04,  6.92it/s]\u001b[A\n",
      " 34%|███▍      | 17/50 [00:02<00:04,  6.93it/s]\u001b[A\n",
      " 36%|███▌      | 18/50 [00:02<00:04,  6.92it/s]\u001b[A\n",
      " 38%|███▊      | 19/50 [00:02<00:04,  6.92it/s]\u001b[A\n",
      " 40%|████      | 20/50 [00:02<00:04,  6.92it/s]\u001b[A\n",
      " 42%|████▏     | 21/50 [00:03<00:04,  6.91it/s]\u001b[A\n",
      " 44%|████▍     | 22/50 [00:03<00:04,  6.91it/s]\u001b[A\n",
      " 46%|████▌     | 23/50 [00:03<00:03,  6.91it/s]\u001b[A\n",
      " 48%|████▊     | 24/50 [00:03<00:03,  6.92it/s]\u001b[A\n",
      " 50%|█████     | 25/50 [00:03<00:03,  6.92it/s]\u001b[A\n",
      " 52%|█████▏    | 26/50 [00:03<00:03,  6.93it/s]\u001b[A\n",
      " 54%|█████▍    | 27/50 [00:03<00:03,  6.93it/s]\u001b[A\n",
      " 56%|█████▌    | 28/50 [00:04<00:03,  6.91it/s]\u001b[A\n",
      " 58%|█████▊    | 29/50 [00:04<00:03,  6.90it/s]\u001b[A\n",
      " 60%|██████    | 30/50 [00:04<00:02,  6.90it/s]\u001b[A\n",
      " 62%|██████▏   | 31/50 [00:04<00:02,  6.88it/s]\u001b[A\n",
      " 64%|██████▍   | 32/50 [00:04<00:02,  6.88it/s]\u001b[A\n",
      " 66%|██████▌   | 33/50 [00:04<00:02,  6.87it/s]\u001b[A\n",
      " 68%|██████▊   | 34/50 [00:04<00:02,  6.87it/s]\u001b[A\n",
      " 70%|███████   | 35/50 [00:05<00:02,  6.86it/s]\u001b[A\n",
      " 72%|███████▏  | 36/50 [00:05<00:02,  6.86it/s]\u001b[A\n",
      " 74%|███████▍  | 37/50 [00:05<00:01,  6.85it/s]\u001b[A\n",
      " 76%|███████▌  | 38/50 [00:05<00:01,  6.85it/s]\u001b[A\n",
      " 78%|███████▊  | 39/50 [00:05<00:01,  6.87it/s]\u001b[A\n",
      " 80%|████████  | 40/50 [00:05<00:01,  6.87it/s]\u001b[A\n",
      " 82%|████████▏ | 41/50 [00:05<00:01,  6.87it/s]\u001b[A\n",
      " 84%|████████▍ | 42/50 [00:06<00:01,  6.88it/s]\u001b[A\n",
      " 86%|████████▌ | 43/50 [00:06<00:01,  6.88it/s]\u001b[A\n",
      " 88%|████████▊ | 44/50 [00:06<00:00,  6.87it/s]\u001b[A\n",
      " 90%|█████████ | 45/50 [00:06<00:00,  6.88it/s]\u001b[A\n",
      " 92%|█████████▏| 46/50 [00:06<00:00,  6.87it/s]\u001b[A\n",
      " 94%|█████████▍| 47/50 [00:06<00:00,  6.88it/s]\u001b[A\n",
      " 96%|█████████▌| 48/50 [00:06<00:00,  6.88it/s]\u001b[A\n",
      " 98%|█████████▊| 49/50 [00:07<00:00,  6.89it/s]\u001b[A\n",
      "100%|██████████| 50/50 [00:07<00:00,  6.89it/s]\u001b[A\n",
      " 75%|███████▌  | 2258/3000 [1:43:55<33:38,  2.72s/it, loss=0.165]     IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weight_dtype = model.model.dtype\n",
    "model.clip_model.to(weight_dtype)\n",
    "\n",
    "progress_bar_epochs = tqdm(range(1, epochs + 1))\n",
    "\n",
    "for epoch in progress_bar_epochs:\n",
    "    model.text_encoder.train()\n",
    "    model.clip_model.train()\n",
    "    for batch in train_dataloader:\n",
    "        model_kwargs = {}\n",
    "        # Convert images to latent representation and add noise\n",
    "        latents = model.image_encoder.encode(batch[\"image\"].to(device=device, dtype=weight_dtype)).detach()\n",
    "        latents = latents * model.scale\n",
    "\n",
    "        noise = torch.randn_like(latents)\n",
    "\n",
    "        timesteps = torch.randint(0, 1000, (batch_size,), device=latents.device)\n",
    "        timesteps = timesteps.long()\n",
    "\n",
    "        noisy_latents = add_noise(latents, noise, timesteps).to(weight_dtype)\n",
    "        \n",
    "        # Get hidden parameters for both models\n",
    "        # First Model Parameters\n",
    "        image_emb = generate_clip_emb(\n",
    "            model,\n",
    "            batch[\"text\"][0],\n",
    "            batch_size=batch_size,\n",
    "            prior_cf_scale=4,\n",
    "            prior_steps=\"5\",\n",
    "            negative_prior_prompt=\"\",\n",
    "        )\n",
    "        \n",
    "        model_kwargs[\"image_emb\"] = image_emb.to(weight_dtype)\n",
    "        \n",
    "        # Second Model Parameters\n",
    "        tokens = model.tokenizer1(\n",
    "            batch[\"text\"][0],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=77,\n",
    "            return_attention_mask=True,\n",
    "            add_special_tokens=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        model_kwargs[\"full_emb\"], model_kwargs[\"pooled_emb\"] = model.text_encoder(\n",
    "            tokens=tokens['input_ids'].long().to(device=device), \n",
    "            mask=tokens['attention_mask'].to(device=device),\n",
    "        )\n",
    "\n",
    "        model_kwargs[\"full_emb\"] = model_kwargs[\"full_emb\"].to(weight_dtype) \n",
    "        model_kwargs[\"pooled_emb\"] = model_kwargs[\"pooled_emb\"].to(weight_dtype) \n",
    "        \n",
    "        # Predict noise obviously\n",
    "        model_pred = model.model(noisy_latents, timesteps, **model_kwargs)[:, :4]\n",
    "\n",
    "        loss = F.mse_loss(model_pred.float(), noise.float(), reduction=\"mean\")\n",
    "\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # We don't need update all embeddings weights. Only new embeddings.\n",
    "        with torch.no_grad():\n",
    "            index_no_updates_t1 = torch.arange(len(model.tokenizer1)) != t1_place_token_id\n",
    "            model.text_encoder.model.transformer.get_input_embeddings().weight[\n",
    "                index_no_updates_t1\n",
    "            ] = orig_t1_params[index_no_updates_t1]\n",
    "            \n",
    "            index_no_updates_t2 = torch.arange(model.clip_model.token_embedding.weight.shape[0]) != t2_place_token_id\n",
    "            model.clip_model.token_embedding.weight[\n",
    "                index_no_updates_t2\n",
    "            ] = orig_t2_params[index_no_updates_t2]\n",
    "            \n",
    "        progress_bar_epochs.set_postfix(**{\"loss\": loss.cpu().detach().item()})\n",
    "    \n",
    "    if epoch % log_embed_frequency == 0:\n",
    "        embed_save_path = os.path.join(out_folder, f\"{epoch}_epoch_embeds.bin\")\n",
    "        save_embeds(model, embed_save_path, placeholder_token, t1_place_token_id, t2_place_token_id)\n",
    "        \n",
    "    if log_image_frequency > 0 and (epoch % log_image_frequency == 0):\n",
    "        images_root = os.path.join(out_folder, \"images\")\n",
    "        os.makedirs(images_root, exist_ok=True)\n",
    "        image_save_path = os.path.join(images_root, f\"{epoch}_epoch_images.jpg\")\n",
    "        save_images(model, image_save_path, placeholder_token)\n",
    "        \n",
    "        \n",
    "embed_save_path = os.path.join(out_folder, \"khinkali_embeds.bin\")\n",
    "save_embeds(model, embed_save_path, placeholder_token, t1_place_token_id, t2_place_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9029bf5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6e0f3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6f3a74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Test_env",
   "language": "python",
   "name": "test_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
